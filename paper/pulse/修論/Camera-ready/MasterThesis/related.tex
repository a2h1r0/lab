\chapter{Related Work}
\label{sec:related}
In this chapter, I introduce research on sensing with wearable devices, the use of smartwatches, and the use of pulse data.

% 2.1
\section{Sensing with Wearable Devices}
There is much research on wearable devices that are worn on body parts, and devices of various shapes have been investigated. Ham et al. \cite{smart_wristband} proposed a wristband-type device as an input device for smart glasses. The device is equipped with a touch panel and an inertial measurement unit, and it can be operated by touch or with a motion such as a twist of the wrist. Because the device is simply worn on the wrist, it offers a high degree of freedom by not restricting the user's movement. A touch panel is used for pointing to improve the input stability. Hernandez et al. \cite{bioglass} proposed a method for acquiring the pulse rate and respiration rate from data obtained from the accelerometer, gyroscope, and camera built into Google Glass, a head-mounted wearable device. Nishajith et al. \cite{smart_cap} designed and implemented ``Smart Cap,'' a wearable device to assist the visually impaired with situational awareness. The device consists of a Raspberry Pi 3, a Raspberry Pi NoIR Camera v2 (an infrared camera module for the Raspberry Pi), an earphone, and a power supply. The infrared camera obtains an image, and the object detected in the image is described by voice through the earphone.\par

For a non-optical approach, Bello et al. \cite{MoCapaci} proposed a wearable system that detects body postures and gestures without requiring sensors to be firmly fixed to the body or integrated into a tight-fitting garment. They implemented a prototype, ``MoCaBlazer,'' by using a standard men's blazer, and they conducted evaluation experiments with 14 subjects. For recognition of 20 actions, the system achieved average recognition accuracy of 97.18\% for leave-one-recording-out evaluation and 86.25\% for user-independent recognition.\par

Among research on other sensor modalities, Vargas et al. \cite{Brainwear} developed an open-source electroencephalography (EEG) sensing module with a state-of-the-art analog front end that is pin- and protocol-compatible with popular ecosystems in the wearable and DIY communities. The goal was to facilitate broad use of EEG sensing in multimodal smart garments. They conducted an evaluation experiment with a proof-of-concept application of the system in a normal baseball cap. They concluded that the system achieved similar levels of recognition to those in other neuroscience studies with dedicated instruments. R\"{o}ddiger et al. \cite{earables} conducted a study with seven different commercially available ``earables'' that are targeted at daytime usage: to investigate their comfort and wearability during sleep, they were all worn by 14 study participants. The results showed that devices occupying more space in the outer ear canal with rigid parts are less desirable. Vekemans et al. \cite{MOTUS} implemented ``MOTUS,'' a prototype watch-back tactile display that conveys emotions, to explore the potential for emotional expression by applying tactile texture patterns to the wrist. A preliminary guessability study with the prototype showed agreement between the texture patterns and the users' interpretation of emotions. Zhou et al. \cite{CoRSA} developed ``CoRSA,'' a lightweight system that supplements existing sports apparel having cardiorespiratory monitoring capabilities with system-in-package (SiP) and system-on-chip (SoC) sensors, which are popular in the wearable computing community. Other studies have examined wearable devices based on rings \cite{wearable_ring1, wearable_ring2, TypingRing, ElectroRing}, belts \cite{wearable_belt1, SmartBelt, WaistonBeltX, wearable_belt2}, and masks \cite{wearable_mask1, wearable_mask2, SilentMask, Masquare}.\par

Various body parts have been investigated as locations for wearable devices, and researchers have sought to estimate the locations from sensor data. Vahdatpour et al. \cite{localization_vahdatpour} collected acceleration data during daily activities from 25 subjects who wore accelerometers at 10 locations on the forearm, upper arm, head, thigh, shin, and waist. From the collected data, a support vector machine (SVM) was able to estimate the attachment site with an average accuracy of 89\%. Sztyler et al. \cite{localization_sztyler} collected acceleration data during various physical activities from 15 subjects who had accelerometers attached to seven locations: the head, chest, left upper arm, left wrist, waist, left pants pocket, and left ankle. From the collected data, the attachment site was estimated with an average accuracy of 89\% by using a random forest. Kunze et al. \cite{localization_kunze} collected data during walking movements from six subjects who had accelerometers attached to four locations: the wrist, the right side of the head above the eye, the left pants pocket, and the left breast pocket. From the collected data, the attachment site was estimated using the C4.5 classifier. In addition, Yoshida et al. \cite{localization_yoshida} proposed a method to estimate the body part where a wearable device is attached without requiring the wearer to perform a specific action; instead, they used electrocardiography (ECG) and pulse data, which are biometric information that can be acquired by the wearable device.


% 2.2
\section{Studies on Smartwatches}
Among wearable devices, smartwatches have long been commercially available, and there is much research on them. Spinsante et al. \cite{accuracy_in_low_intensity} studied the heart rate obtained from a smartwatch during low-intensity physical activity and measured its accuracy. Sen et al. \cite{eating_recognition} proposed a method to record a user's eating behavior, such as the use of hands, chopsticks, or a spoon, by using data obtained from a smartwatch's accelerometer and gyroscope. By capturing food images with the smartwatch's built-in camera and performing image identification, the meal contents were also recorded. In another study, by leveraging the fact that smartwatches are always worn at the same location and in the same direction, Johnston et al. \cite{smartwatch_walk_authentication} proposed a method for biometric authentication based on gait data obtained from a smartwatch's accelerometer and gyroscope.\par

Smartphones are typically carried in a pants pocket or handbag, but compared to those locations, more activity information tends to be available at the wrist, where smartwatches are worn. Weiss et al. \cite{smartwatch_activity_recognition} showed that a smartwatch can identify actions more effectively than a smartphone in hand-based physical behaviors such as eating. The smartwatch could identify the behavior of ``drinking'' with 93.3\% accuracy, while the smartphone could only achieve 77.3\% accuracy.\par

Among other potential applications, Iakovakis et al. \cite{oh_detection} conducted a study on using a smartwatch to predict blood pressure drops due to postural changes. Orthostatic hypotension (OH) has been shown to cause dizziness and fainting and is a risk factor for falls in the young as well as the elderly. Accordingly, they proposed a mathematical prediction model that can reduce the risk of falls due to OH by sensing heart rate variability. Mauldin et al. \cite{smartfall} proposed an Android application, ``SmartFall,'' that detects falls by using acceleration data obtained from a commercially available smartwatch. The smartwatch is paired with a smartphone that runs the software. SmartFall communicates with a cloud server to perform the calculations necessary to predict falls in real time, while maintaining data privacy. Ciabattoni et al. \cite{smartwatch_stress_detection} proposed a method for detecting mental stress during various cognitive tasks in real time. Stress is classified by using galvanic skin response (GSR), RR interval (i.e., the time between successive R-peaks in an ECG), and body temperature data acquired by a commercial smartwatch. Sun et al. \cite{SleepMonitor} developed ``SleepMonitor,'' a smartwatch-based system for monitoring the user's respiratory rate and body position. The system uses accelerometer data collected at the wrist to estimate the respiratory rate. The results of evaluation experiments showed that the system could monitor the respiratory rate and body position during sleep with high accuracy under various conditions.\par

On the other hand, for an artificial limb, wearable devices cannot collect biometric information. In that case, methods using sensors such as accelerometers and gyroscopes are applicable, but methods using biometric data are not. Accordingly, I seek to make these applications available to users with artificial limbs as well as living limbs by inputting suitable data to the biometric sensors of wearable devices.


% 2.3
\section{Studies on Pulse Data}
Havriushenko et al. \cite{respiratory_rate_estimation1} proposed a method for estimating a user's respiratory rate from pulse wave data by using neural networks. The respiratory rate is often measured with a thermal sensor placed in the nasal channels or an elastic chest belt, but these devices may interfere with sleep. In contrast, their method can be implemented in a wearable device. The results of their evaluation showed an average respiratory rate estimation error lower than 2.2 breaths per minute. Jarchi et al. \cite{respiratory_rate_estimation2} proposed a method that relies on a nonlinear time-frequency representation, called the wavelet synchrosqueezed transform (WSST), to estimate the instantaneous respiratory rate from body-mounted PPG sensors. Han et al. \cite{arrhythmia_detection} proposed a method for detecting premature atrial contraction and premature ventricular contraction by using PPG data acquired from a smartwatch. Wang et al. \cite{alcohol_detection} developed a system for identifying excess alcohol consumption by using an SVM with data from ECG and PPG monitoring. Longmore et al. \cite{ppg_location} sought to identify a single location in the human anatomy for measuring simultaneously the heart rate (HR), blood oxygen saturation (SpO2), and respiration rate at rest and while walking by a single PPG sensor. In addition, Okamoto et al. proposed a method to recognize the arm's muscle activity state and a method to estimate a surface electromyogram (sEMG) from PPG data \cite{semg_okamoto}. The results of an evaluation experiment with five participants showed that three types of muscle activity were recognized with over 75\% accuracy, and the sEMG was estimated with an error of approximately 20\%.\par

Among potential pulse data applications related to emotions, Goshvarpour et al. \cite{emotion_recognition1} proposed a method for classifying emotional responses by means of a simple dynamic signal processing technique and a fusion framework. They recorded the ECG and finger pulse activity of 35 subjects during a rest condition and when the subjects were listening to music that was intended to stimulate certain emotions. After constructing Poincar\'e plots, an SVM was used to classify them into four emotions: happiness, sadness, peacefulness, and fear. Kajiwara et al. \cite{emotion_recognition2} developed an application for logistics companies that adopt a manual order picking system, given that emotions and engagement affect work efficiency and human errors. Specifically, they proposed a method for predicting emotions and engagement during work with a high exercise intensity from behavior and pulse wave data acquired by wearable devices. Pulse wave, eye movement, and general movement data are input to deep neural networks to estimate a worker's emotion and engagement. The results of verification experiments showed that the emotion and engagement during order picking could be accurately predicted from the worker's behavior with an error rate of 0.12 or less. Lee et al. \cite{emotion_recognition3} conducted research on improving the speed of emotion recognition by using a PPG signal. A two-dimensional emotion model based on valence and arousal was adopted, and a one-dimensional convolutional neural network (1D CNN) was used to recognize emotions from a 1.1-s PPG signal. The 1D CNN was tested as a binary classifier (high or low valence and arousal) by using the dataset for emotion analysis using physiological signals (DEAP), and it achieved recognition accuracies of 75.3\% for valence and 76.2\% for arousal. Udovi\v{c}i\'{c} et al. \cite{emotion_recognition4} studied emotion recognition using only GSR and PPG signals because of their suitability for implementation in a simple wearable device that can collect signals from a person without compromising comfort and privacy. In addition to the above studies, there have been many others on the use of PPG data for emotion recognition \cite{emotion_recognition5, emotion_recognition6, emotion_recognition7}.\par

Pulse data is one of the most important pieces of biological information, as it can be applied to detect abnormalities in the body and recognize emotions. Most pulse sensors in commercially available wearable devices use PPG. As a result, when a wearable device is mounted on an artificial limb, where there is no blood flow, pulse data cannot be acquired. Hence, among various kinds of biometric data, I focus on pulse data and propose a method to allow a wearable device to measure pulse data similar to that of a living limb even on an artificial limb.
